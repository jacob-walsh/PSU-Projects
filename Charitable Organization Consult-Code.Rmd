---
title: "Final Project draft - Part 1: Classification"
author: "Jacob Walsh"
date: "December 4, 2016"
output: word_document
---
```{r}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=65),tidy=TRUE, cache=TRUE)
```

```{r eval=FALSE}
charity <- read.csv("C:/Penn State MAS/Stat 897D/charity.csv")
#Exploratory
hist(charity$chld)
hist(charity$hinc)
hist(charity$wrat)
hist(charity$avhv)
hist(charity$incm)
hist(charity$inca)
hist(charity$plow)
hist(charity$npro)
hist(charity$tgif)
hist(charity$lgif)
hist(charity$rgif)
hist(charity$tdon)
hist(charity$tlag)
hist(charity$agif)

#non normality in predictors may lead to unreliable results

hist(log(charity$avhv))
hist(log(charity$incm))
hist(log(charity$inca))
hist(log(charity$plow))
hist(log(charity$npro))
hist(log(charity$tgif))
hist(log(charity$lgif))
hist(log(charity$rgif))
hist(log(charity$tdon))
hist(log(charity$tlag))
hist(log(charity$agif))
#transform data - log transformation seems to make most considerably more normal except for npro, leave npro as is.
```
```{r eval=FALSE}
charity <- read.csv("C:/Penn State MAS/Stat 897D/charity.csv")
#transform data
t.charity=charity
t.charity$avhv=log(charity$avhv)
t.charity$incm=log(charity$incm)
t.charity$inca=log(charity$inca)
t.charity$plow=charity$plow^(1/3)
t.charity$tgif=log(charity$tgif)
t.charity$lgif=log(charity$lgif)
t.charity$rgif=log(charity$rgif)
t.charity$tdon=log(charity$tdon)
t.charity$tlag=log(charity$tlag)
t.charity$agif=log(charity$agif)

#partition data
data.train <- t.charity[t.charity$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train)


data.valid <- t.charity[t.charity$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid)


data.test <- t.charity[t.charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]

x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
```
## Subset selection
```{r eval=FALSE}
library(MASS)
library(leaps)
model.glm=glm(donr~., data.train.std.c, family='binomial')
summary(model.glm)

#lasso
library(glmnet)
grid=10^seq(10,-2,length=100)
xtrain=model.matrix(donr~., data=data.train.std.c)
ytrain=data.train.std.c$donr
xtest=model.matrix(donr~., data.valid.std.c)
ytest=data.valid.std.c$donr

model.lasso=glmnet(xtrain, ytrain, alpha=1, lambda=grid, family='binomial')
cv.lasso=cv.glmnet(xtrain,ytrain,alpha=1, family='binomial')
lambda=cv.lasso$lambda.min
lasso.pred=predict(model.lasso, s=lambda, newx=xtest)
mean((lasso.pred-ytest)^2)
lasso.coef=predict(model.lasso, type="coefficients", s=lambda)
lasso.coef

```

## Model Comparison

###Logistic 
1-Full 
2-Subset selected through lasso and signficance of predictors combined with guess and checking to increase profit
```{r eval=FALSE}
model.log1 = glm(donr~., data=data.train.std.c, family=binomial("logit"))
post.valid.log1=predict(model.log1, data.valid.std.c, type='response')
profit.log1 <- cumsum(14.5*c.valid[order(post.valid.log1, decreasing=T)]-2)
plot(profit.log1) # see how profits change as more mailings are made
n.mail.valid1 <- which.max(profit.log1) # number of mailings that maximizes profits
c(n.mail.valid1, max(profit.log1))

model.log2 = glm(donr~reg1+reg2+home+chld+wrat+incm+tgif+tdon+tlag +I(hinc^2)+I(tgif^2), data=data.train.std.c, family=binomial("logit"))
post.valid.log2=predict(model.log2, data.valid.std.c, type='response')
profit.log2 <- cumsum(14.5*c.valid[order(post.valid.log2, decreasing=T)]-2)
plot(profit.log2) # see how profits change as more mailings are made
n.mail.valid2 <- which.max(profit.log2) # number of mailings that maximizes profits
c(n.mail.valid2, max(profit.log2))

model.log3 = glm(donr~.+I(hinc^2), data=data.train.std.c, family=binomial("logit"))
post.valid.log3=predict(model.log3, data.valid.std.c, type='response')
profit.log3 <- cumsum(14.5*c.valid[order(post.valid.log3, decreasing=T)]-2)
plot(profit.log3) # see how profits change as more mailings are made
n.mail.valid3 <- which.max(profit.log3) # number of mailings that maximizes profits
c(n.mail.valid3, max(profit.log3))
```

###LDA  1-Full 2-Subset
```{r eval=FALSE}
library(MASS)
model.lda1=lda(donr~.+I(hinc^2), data=data.train.std.c)
post.valid.lda1 <- predict(model.lda1, data.valid.std.c)$posterior[,2]

profit.lda1 <- cumsum(14.5*c.valid[order(post.valid.lda1, decreasing=T)]-2)
plot(profit.lda1) # see how profits change as more mailings are made
n.mail.valid4 <- which.max(profit.lda1) # number of mailings that maximizes profits
c(n.mail.valid4, max(profit.lda1)) # report number of mailings and maximum profit

cutoff.lda1 <- sort(post.valid.lda1, decreasing=T)[n.mail.valid4+1] # set cutoff based on n.mail.valid
chat.valid.lda1 <- ifelse(post.valid.lda1>cutoff.lda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda1, c.valid) # classification table


model.lda2=lda(donr~reg1+reg2+home+chld+wrat+incm+tgif+tdon+tlag +I(hinc^2)+I(tgif^2), data=data.train.std.c)

post.valid.lda2 <- predict(model.lda2, data.valid.std.c)$posterior[,2]

profit.lda2<- cumsum(14.5*c.valid[order(post.valid.lda2, decreasing=T)]-2)
plot(profit.lda2) # see how profits change as more mailings are made
n.mail.valid5 <- which.max(profit.lda2) # number of mailings that maximizes profits
c(n.mail.valid5, max(profit.lda2)) # report number of mailings and maximum profit

cutoff.lda2 <- sort(post.valid.lda2, decreasing=T)[n.mail.valid5+1] # set cutoff based on n.mail.valid
chat.valid.lda2 <- ifelse(post.valid.lda2>cutoff.lda2, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda2, c.valid) # classification table
```


###QDA  1-Full 2-Subset
```{r eval=FALSE}

model.qda1=qda(donr~., data=data.train.std.c)
post.valid.qda1 <- predict(model.qda1, data.valid.std.c)$posterior[,2]

profit.qda1 <- cumsum(14.5*c.valid[order(post.valid.qda1, decreasing=T)]-2)
plot(profit.qda1) # see how profits change as more mailings are made
n.mail.valid6 <- which.max(profit.qda1) # number of mailings that maximizes profits
c(n.mail.valid6, max(profit.qda1)) # report number of mailings and maximum profit

cutoff.qda1 <- sort(post.valid.qda1, decreasing=T)[n.mail.valid6+1] # set cutoff based on n.mail.valid
chat.valid.qda1 <- ifelse(post.valid.qda1>cutoff.qda1, 1, 0) # mail to everyone above the cutoff
table(chat.valid.qda1, c.valid) # classification table


model.qda2=qda(donr~reg1+reg2+home+chld+wrat+incm+tgif+tdon+tlag +I(hinc^2)+I(tgif^2), data=data.train.std.c)
post.valid.qda2 <- predict(model.qda2, data.valid.std.c)$posterior[,2]

profit.qda2 <- cumsum(14.5*c.valid[order(post.valid.qda2, decreasing=T)]-2)
plot(profit.qda2) # see how profits change as more mailings are made
n.mail.valid7 <- which.max(profit.qda2) # number of mailings that maximizes profits
c(n.mail.valid7, max(profit.qda2)) # report number of mailings and maximum profit

cutoff.qda2 <- sort(post.valid.qda2, decreasing=T)[n.mail.valid7+1] # set cutoff based on n.mail.valid
chat.valid.qda2 <- ifelse(post.valid.qda2>cutoff.qda2, 1, 0) # mail to everyone above the cutoff
table(chat.valid.qda2, c.valid) # classification table
```

###KNN
```{r eval=FALSE}
library(class)
set.seed(1)
post.valid.knn5=knn(x.train.std,x.valid.std,c.train,k=5)
profit.knn5<- cumsum(14.5*c.valid[order(post.valid.knn5, decreasing=T)]-2)
plot(profit.knn5)
n.mail.valid8=which.max(profit.knn5)
c(n.mail.valid8, max(profit.knn5))

set.seed(1)
post.valid.knn10=knn(x.train.std,x.valid.std,c.train,k=10)
profit.knn10<- cumsum(14.5*c.valid[order(post.valid.knn10, decreasing=T)]-2)
plot(profit.knn10)
n.mail.valid9=which.max(profit.knn10)
c(n.mail.valid9, max(profit.knn10))

set.seed(1)
post.valid.knn20=knn(x.train.std,x.valid.std,c.train,k=20)
profit.knn20<- cumsum(14.5*c.valid[order(post.valid.knn20, decreasing=T)]-2)
plot(profit.knn20)
n.mail.valid10=which.max(profit.knn20)
c(n.mail.valid10, max(profit.knn20))
```
###GAM
```{r eval=FALSE}
library(gam)
model.gam1=gam(donr~reg1+reg2+home+chld+wrat+avhv+incm+tlag+tgif+s(tdon,df=3)+s(hinc,df=5), data=data.train.std.c, family=binomial)

post.valid.gam1<-predict(model.gam1,data.valid.std.c,type='response')
profit.gam1<- cumsum(14.5*c.valid[order(post.valid.gam1, decreasing=T)]-2)
plot(profit.gam1)
n.mail.valid11=which.max(profit.gam1)
c(n.mail.valid11, max(profit.gam1))


model.gam2=gam(donr~reg1+reg2+home+chld+wrat+s(hinc,df=3)+s(incm,df=3)+s(tgif,df=3)+s(tdon,df=3)+s(tlag,df=3), data=data.train.std.c, family=binomial)

post.valid.gam2<-predict(model.gam2,data.valid.std.c,type='response')
profit.gam2<- cumsum(14.5*c.valid[order(post.valid.gam2, decreasing=T)]-2)
plot(profit.gam2)
n.mail.valid12=which.max(profit.gam2)
c(n.mail.valid12, max(profit.gam2))

model.gam3=gam(donr~reg1+reg2+home+chld+wrat+s(hinc,df=5)+s(incm,df=5)+s(tgif,df=5)+s(tdon,df=5)+s(tlag,df=5), data=data.train.std.c, family=binomial)

post.valid.gam3<-predict(model.gam3,data.valid.std.c,type='response')
profit.gam3<- cumsum(14.5*c.valid[order(post.valid.gam3, decreasing=T)]-2)
plot(profit.gam3)
n.mail.valid13=which.max(profit.gam3)
c(n.mail.valid13, max(profit.gam3))

#having df of 5 across the board increases the profit but at a risk of overfitting, there isn't very much difference between gam3 and gam1
```

###SVM
```{r eval=FALSE}
library(e1071)
#model.svm1=tune(svm, donr~., data=data.train.std.c, ranges = list(cost = c(0.01, 0.1, 1, 10)), kernel = "linear")

##tried svm, but took so long that is is impractical.
```

###Trees
```{r eval=FALSE}
library(caret)
set.seed(1)
gbmfit<- train(as.factor(donr)~., data=data.train.std.c, method='gbm')
```

##Optimal GBM
Using the package caret the parameters for the gradient boosting model were tuned to the following:
```{r eval=FALSE}
gbmfit
set.seed(1)
boost.mod=gbm(donr~.,data=data.train.std.c, distribution="bernoulli", n.trees=150, interaction.depth=3, shrinkage=0.1, n.minobsinnode=10)

post.valid.boost<-predict(boost.mod,data.valid.std.c,type='response', n.trees=150)
profit.boost<- cumsum(14.5*c.valid[order(post.valid.boost, decreasing=T)]-2)
plot(profit.boost)
n.mail.valid14=which.max(profit.boost)
c(n.mail.valid14, max(profit.boost))
```

##Summary
```{r eval=FALSE}
#model 1 - full model w/all predictors
#model 2-subset model w/reg1+reg2+home+chld+wrat+incm+tgif+tdon+tlag +I(hinc^2)+I(tgif^2)
c(n.mail.valid1, max(profit.log1))
c(n.mail.valid2, max(profit.log2))
c(n.mail.valid3, max(profit.log3)) #full model + hinc^2
c(n.mail.valid4, max(profit.lda1)) #full model + hinc^2
c(n.mail.valid5, max(profit.lda2))
c(n.mail.valid6, max(profit.qda1))
c(n.mail.valid7, max(profit.qda2))
c(n.mail.valid8, max(profit.knn5))
c(n.mail.valid9, max(profit.knn10))
c(n.mail.valid10, max(profit.knn20))
c(n.mail.valid11, max(profit.gam1)) #best model gam(donr~reg1+reg2+home+chld+wrat+avhv+incm+tlag+tgif+s(tdon,df=3)+s(hinc,df=5), data=data.train.std.c, family=binomial)
c(n.mail.valid12, max(profit.gam2))
c(n.mail.valid13, max(profit.gam3))
c(n.mail.valid14, max(profit.boost))

```


#Part 2 Prediction
```{r eval=FALSE}
charity <- read.csv("C:/Penn State MAS/Stat 897D/charity.csv")
#transform data
t.charity=charity
t.charity$avhv=log(charity$avhv)
t.charity$incm=log(charity$incm)
t.charity$inca=log(charity$inca)
t.charity$plow=charity$plow^(1/3)
t.charity$tgif=log(charity$tgif)
t.charity$lgif=log(charity$lgif)
t.charity$rgif=log(charity$rgif)
t.charity$tdon=log(charity$tdon)
t.charity$tlag=log(charity$tlag)
t.charity$agif=log(charity$agif)


#partition data
data.train <- t.charity[t.charity$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train)


data.valid <- t.charity[t.charity$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid)


data.test <- t.charity[t.charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]

x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
```

##Model Comparison
```{r eval=FALSE}

library(leaps)
model.ls1 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)

pred.valid.ls1 <- predict(model.ls1, newdata = data.valid.std.y) # validation predictions
lm.mse=mean((y.valid - pred.valid.ls1)^2) # mean prediction error
sd((y.valid - pred.valid.ls1)^2)/sqrt(n.valid.y) # std error

#best subsets
model.bestsub=regsubsets(damt~., data=data.train.std.y, nvmax=20)
test.mse=model.matrix(damt~.,data=data.valid.std.y)
val.errors=rep(NA,20)
for(i in 1:20){
coefi=coef(model.bestsub,id=i)
pred=test.mse[,names(coefi)]%*%coefi
val.errors[i]=mean((data.valid.std.y$damt-pred)^2)
}
val.errors[which.min(val.errors)]
which.min(val.errors)
coefficients(model.bestsub, which.min(val.errors)) #Coefficients for the "best" model.  There is no reduction in the amount of predictors to minimize the mean prediction error, all 20 predictors are included.

plot(model.bestsub, scale="bic")

model.bestfwd=regsubsets(damt~., data=data.train.std.y, method="forward")
plot(model.bestfwd, scale="bic")

model.bestbwd=regsubsets(damt~., data=data.train.std.y, nvmax=20, method="backward")
plot(model.bestbwd, scale="bic")

#bic for best subsets and backward are the same 10 predictors reg3+reg4+home+chld+hinc+incm+tgif+lgif+rgif+agif
lm8.mse=val.errors[8]
#bic for the best foward regressions is minimized at 8 predictors
lm10.mse=val.errors[10]
#mSE isn't any lower
```

###Ridge and Lasso
```{r eval=FALSE}
library(glmnet)
grid=10^seq(10,-2,length=100)
xtrain=model.matrix(damt~., data=data.train.std.y)
ytrain=data.train.std.y$damt
xtest=model.matrix(damt~., data.valid.std.y)
ytest=data.valid.std.y$damt

model.ridge=glmnet(xtrain,ytrain,alpha=0, lambda=grid)
cv.ridge=cv.glmnet(xtrain,ytrain,alpha=0)
pred.ridge=predict(model.ridge, s=cv.ridge$lambda.min, newx=xtest)
ridge.mse=mean((pred.ridge-ytest)^2)

model.lasso=glmnet(xtrain,ytrain,alpha=1, lambda=grid)
cv.lasso=cv.glmnet(xtrain,ytrain,alpha=1)
pred.lasso=predict(model.lasso, s=cv.lasso$lambda.min, newx=xtest)
lasso.mse=mean((pred.lasso-ytest)^2)

#ridge and lasso aren't performing any better than plain old least squares.
```

###PCR and PLS
```{r eval=FALSE}
#pcr
library(pls)
set.seed(1)
pcr.fit=pcr(damt~.,data=data.train.std.y, scale=TRUE, validation = "CV")
validationplot(pcr.fit,val.type="MSEP")

#not much gained after 5, next slight dip is at 13.
pcr.pred=predict(pcr.fit,data.valid.std.y, ncomp=5)
pcr5.mse=mean((pcr.pred-y.valid)^2)
pcr.pred=predict(pcr.fit,data.valid.std.y, ncomp=13)
pcr13.mse=mean((pcr.pred-y.valid)^2)

pls.fit=plsr(damt~.,data=data.train.std.y, scale=TRUE, validation="CV")
validationplot(pls.fit, val.type="MSEP")

pls.pred=predict(pls.fit,data.valid.std.y, ncomp=2)
pls.mse=mean((pls.pred-y.valid)^2)  #slight decrease in mse, with only 2 components..
```

###SVM

```{r eval=FALSE}
library(e1071)
svmfit1=svm(damt~., data=data.train.std.y, cost =1, gamma=0.01, kernel = "radial")
svmfit2=svm(damt~., data=data.train.std.y, cost =1, gamma=0.01, epsilon=.2, kernel = "radial") 

svmpred1=predict(svmfit1, newdata=data.valid.std.y)
svmpred2=predict(svmfit2, newdata=data.valid.std.y)

svm1.mse=mean((svmpred1-y.valid)^2)
svm2.mse=mean((svmpred2-y.valid)^2) #best so far
```

###Random Forests
```{r eval=FALSE}
library(randomForest)
set.seed(1)
bag.charity=randomForest(damt~.,data=data.train.std.y, mtry=20, importance=TRUE)
pred.valid.bag=predict(bag.charity, newdata=data.valid.std.y)
bag.mse=mean((pred.valid.bag-y.valid)^2)

set.seed(1)
rf.charity=randomForest(damt~.,data=data.train.std.y, mtry=10, ntree=25, importance=TRUE)
pred.valid.rf=predict(rf.charity, newdata=data.valid.std.y)
rf.mse=mean((pred.valid.rf-y.valid)^2)
```

###GBM
I started out by manually changing the parameters using the example in ISLR.  After some research, I found a package that does this for you.  It's called caret and it did better than any other model by far.
```{r eval=FALSE}
library(gbm)
boost.mod=gbm(damt~.,data=data.train.std.y, distribution="gaussian", n.trees=3000, interaction.depth=4)
pred.valid.gbm=predict(boost.mod, newdata=data.valid.std.y, n.trees=1000)
gbm1.mse=mean((pred.valid.gbm-y.valid)^2)

boost.mod=gbm(damt~.,data=data.train.std.y, distribution="gaussian", n.trees=5000, interaction.depth=6)
pred.valid.gbm=predict(boost.mod, newdata=data.valid.std.y, n.trees=5000)
gbm2.mse=mean((pred.valid.gbm-y.valid)^2)
```

```{r eval=FALSE}
library(caret)
set.seed(1)
gbmfit<- train(damt~., data=data.train.std.y, method='gbm')
```
####Optimal GBM
Using the package caret I tuned the parameters to the following
```{r eval=FALSE}
gbmfit
set.seed(1)
model.gbm=gbm(damt~.,data=data.train.std.y, distribution="gaussian", n.trees=150, interaction.depth=3, shrinkage=0.1, n.minobsinnode=10)
pred.valid.gbm=predict(model.gbm, newdata=data.valid.std.y, n.trees=150)
gbmoptimal.mse=mean((pred.valid.gbm-y.valid)^2)
```

##Results
```{r eval=FALSE}
lm.mse
lm8.mse
lm10.mse
ridge.mse
lasso.mse
pcr5.mse
pcr13.mse
pls.mse
svm1.mse
svm2.mse
bag.mse
rf.mse
gbmoptimal.mse
```


```{r eval=FALSE}
post.test <- predict(boost.mod, data.test.std, type="response", n.trees=150) # post probs for test data

# Oversampling adjustment for calculating number of mailings for test set

n.mail.valid <- which.max(profit.boost)
tr.rate <- .1 # typical response rate is .1
vr.rate <- .5 # whereas validation response rate is .5
adj.test.1 <- (n.mail.valid/n.valid.c)/(vr.rate/tr.rate) # adjustment for mail yes
adj.test.0 <- ((n.valid.c-n.mail.valid)/n.valid.c)/((1-vr.rate)/(1-tr.rate)) # adjustment for mail no
adj.test <- adj.test.1/(adj.test.1+adj.test.0) # scale into a proportion
n.mail.test <- round(n.test*adj.test, 0) # calculate number of mailings for test set

cutoff.test <- sort(post.test, decreasing=T)[n.mail.test+1] # set cutoff based on n.mail.test
chat.test <- ifelse(post.test>cutoff.test, 1, 0) # mail to everyone above the cutoff
table(chat.test)
yhat.test <- predict(model.gbm, newdata = data.test.std, n.trees=150)

ip <- data.frame(chat=chat.test, yhat=yhat.test) # data frame with two variables: chat and yhat
write.csv(ip, file="C:/Penn State MAS/Stat 897D/TeamB-FinalProject-JW&MG.ip.csv", 
          row.names=FALSE)
```


