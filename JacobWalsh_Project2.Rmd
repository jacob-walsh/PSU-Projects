---
title: "Project 2 - Classification"
author: "Jacob Walsh"
date: "November 5, 2016"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

```{r include=FALSE}
load("C:/Penn State MAS/Stat 897D/Project2.RData")
ls()
Y.train <- Network.train[lower.tri(Network.train)] # 7381 pairs in training

n.train <- length(Y.train)
Y.train.mean <- mean(Y.train)
Y.valid <- Network.valid[lower.tri(Network.valid)] # 1378 pairs in validation

n.valid <- length(Y.valid)
mean(Y.valid)
# 0.03120464 proportion of PPI interactions in validation data
X.train = NULL # 7381 by 5 matrix with 5 predictors for each gene pair
for (i in 1:(dim(DATA.train)[1]-1))
 for (j in (i+1):dim(DATA.train)[1])
 X.train = rbind(X.train,
 c(mean(DATA.train[i,]), mean(DATA.train[j,]),
 cov(DATA.train[i,],DATA.train[j,]),
var(DATA.train[i,]), var(DATA.train[j,])))
data.train <- as.data.frame(cbind(Y.train, X.train))
names(data.train) <- c("Y", "X1", "X2", "X3", "X4", "X5")
X.valid = NULL # 1378 by 5 matrix with 5 predictors for each gene pair
for (i in 1:(dim(DATA.valid)[1]-1))
 for (j in (i+1):dim(DATA.valid)[1])
 X.valid = rbind(X.valid,
  c(mean(DATA.valid[i,]), mean(DATA.valid[j,]),
  cov(DATA.valid[i,],DATA.valid[j,]),
var(DATA.valid[i,]), var(DATA.valid[j,])))
data.valid <- as.data.frame(X.valid)
names(data.valid) <- c("X1", "X2", "X3", "X4", "X5")

```


##Introduction
Gene Expression Data and Physical Interaction Network (Brem and Kruglyak, 2005):
This report classifies protein-protein gene interactions (PPI) in response to Rapamycin. 
There are 231 active genes, 93 individuals and 6 time points measured for each individual.
Predictions for the interacted genes will be done by assessing 5 predictors. X1 - mean of gene i,
X2- mean of gene j, X3 -  covariance between gene i and gene j, X4- variance of gene i,
and X5 - variance of gene j.  The data has been split into a training set of 7381 observations and a
validation set with 1378 observations.   Five models will be looked at: logistic regression model, linear
discriminant analysis (LDA), quadratic discriminant analysis (QDA), K-nearest neighbors, and logistic 
regression generalized additive model (GAM).   Sensitivity, specificity and error rate will be 
calculated for both the training sets and the validation sets.  


##Analysis


```{r include=FALSE}

model.logistic <- glm(Y ~., data.train, family=binomial("logit"))
summary(model.logistic)
post.train.logistic <- model.logistic$fitted.values # n.train posterior probabilities of Y=1

Ghat.train.logistic <-ifelse(post.train.logistic > sort(post.train.logistic, decreasing=T)[201], 1, 0) # classification rule
table(Ghat.train.logistic,Y.train) # classification table

sum(abs(Ghat.train.logistic-Y.train))/n.train # training data classification error rate 
sum(Ghat.train.logistic==1&Y.train==1)/sum(Y.train==1) # sensitivity
sum(Ghat.train.logistic==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.logistic <- predict(model.logistic, data.valid, type="response") # n.valid post probs
Ghat.valid.logistic <- ifelse(post.valid.logistic > sort(post.train.logistic, decreasing=T)[201], 1, 0) 
table(Ghat.valid.logistic,Y.valid)
sum(abs(Ghat.valid.logistic-Y.valid))/n.valid # classification error rate 
sum(Ghat.valid.logistic==1&Y.valid==1)/sum(Y.valid==1) # sensitivity
sum(Ghat.valid.logistic==0&Y.valid==0)/sum(Y.valid==0) # specificity
```


###Logistic Regression
  A logistic model yields the following coefficients:
```{r echo=FALSE, comment=' '}
coef(model.logistic)
```
The predictors X4 and X5 should be noted as not significant based on their resulting
p-values (0.984 and 0.387 respectively).   Results may differ if we do not include those predictors.
The following table compares the predicted
outcomes to the actual outcomes.  The first table shows the results for the training data set and
the second table shows the results when predictions are made on the validation set.  
An observation in class 1 indicates that their posterior probability is greater
than the cutoff probability, which in this case is 0.172. This cut off probability corresponds to the 
201st highest gene pair posterior probability.  Hence, in row 1 for the training data, we have the 200
observations that are the most likely interacted gene pairs.
 
```{r echo=FALSE, comment=' '}
table(Ghat.train.logistic,Y.train)
 table(Ghat.valid.logistic,Y.valid)
summary.log=data.frame(Classification=c("Training Data", "Validation Data"), ErrorRate=c("3.27%", "3.85%"), Sensitivity=c("39.6%","55.8%"), Specificity=c("98.3%", "97.4%"))
summary.log
```


```{r include=FALSE}

library(MASS)
model.lda <- lda(Y ~ ., data.train)
model.lda

post.train.lda <- predict(model.lda)$posterior[,2] # n.train posterior probabilities of Y=1

sort(post.train.lda, decreasing=T)[201]
Ghat.train.lda <- ifelse(post.train.lda > sort(post.train.lda, decreasing=T)[201], 1, 0)  # classification rule

table(Ghat.train.lda,Y.train)
sum(abs(Ghat.train.lda-Y.train))/n.train # training data classification error rate
sum(Ghat.train.lda==1&Y.train==1)/sum(Y.train==1) # sensitivity

sum(Ghat.train.lda==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.lda <- predict(model.lda, data.valid)$posterior[,2] # n.valid posterior probabilities of Y=1
Ghat.valid.lda <- ifelse(post.valid.lda > sort(post.train.lda, decreasing=T)[201], 1, 0) 
table(Ghat.valid.lda,Y.valid) # classification table

sum(abs(Ghat.valid.lda-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.lda==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.lda==0&Y.valid==0)/sum(Y.valid==0) # specificity 
```

###  Linear Discriminant Analysis
The LDA model doesn't perform as well as the logistic regression.  The model sensitivity has decreased from 55.8%
to 51.2%. The cut off posterior probability used was 0.143.
```{r echo=FALSE, comment=' '}
table(Ghat.train.lda,Y.train)
table(Ghat.valid.lda,Y.valid)

summary.lda=data.frame(Classification=c("Training Data", "Validation Data"), ErrorRate=c("3.56%", "3.26%"), Sensitivity=c("34%","51.2%"), Specificity=c("98.1%", "98.2%"))
summary.lda

```

  

```{r include=FALSE}
model.qda <- qda(Y ~ ., data.train)
post.train.qda <- predict(model.qda)$posterior[,2] 
Ghat.train.qda <- ifelse(post.train.qda > sort(post.train.qda, decreasing=T)[201], 1, 0)  # classification rule

table(Ghat.train.qda,Y.train) # classification table

sum(abs(Ghat.train.qda-Y.train))/n.train # training data classification error rate 

sum(Ghat.train.qda==1&Y.train==1)/sum(Y.train==1) # sensitivity 

sum(Ghat.train.qda==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.qda <- predict(model.qda, data.valid)$posterior[,2] # n.valid posterior probabilities of Y=1
Ghat.valid.qda <- ifelse(post.valid.qda > sort(post.train.qda, decreasing=T)[201], 1, 0) 
table(Ghat.valid.qda,Y.valid) # classification table

sum(abs(Ghat.valid.qda-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.qda==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.qda==0&Y.valid==0)/sum(Y.valid==0) # specificity 
```

###Quadratic Discriminant Analysis
The accuracy in the predictions from the QDA are very similar to the results from the logistic regression.  Here
the cut-off posterior probability for the top 200 is 0.35.
```{r echo=FALSE, comment=' '}
table(Ghat.train.qda,Y.train) 
table(Ghat.valid.qda,Y.valid)
summary.qda=data.frame(Classification=c("Training Data", "Validation Data"), ErrorRate=c("3.45%", "3.41%"), Sensitivity=c("36%","55.8%"), Specificity=c("98.2%", "97.9%"))
summary.qda
```



```{r include=FALSE}
library(class)
mer <- rep(NA, 30) # misclassification error rates based on leave-one-out cross-validation

set.seed(2014)

for (i in 1:30) mer[i] <- sum((Y.train-(c(knn.cv(train=X.train, cl=Y.train, k=i))-1))^2)/n.train
plot(mer, ylab="Misclassification Error Rate", xlab="K Value")
points(which.min(mer), mer[which.min(mer)], pch=9, col="red")
which.min(mer) # minimum occurs at k=12
set.seed(2014)
trainmodel.knn <- knn(train=scale(X.train), test=scale(X.train), cl=scale(Y.train), k=12, prob=T)

trainpredclass.knn <- c(trainmodel.knn)-1 # convert factor to numeric classes

trainpredprob.knn <- attr(trainmodel.knn, "prob") # proportion of votes for winning class

post.train.knn <- trainpredclass.knn*trainpredprob.knn+(1-trainpredclass.knn)*(1-trainpredprob.knn)

Ghat.train.knn <- ifelse(post.train.knn > sort(post.train.knn, decreasing=T)[201], 1, 0) # classification rule

table(Ghat.train.knn,Y.train)
sum(abs(Ghat.train.knn-Y.train))/n.train # training data classification error rate
sum(Ghat.train.knn==1&Y.train==1)/sum(Y.train==1) # sensitivity 
sum(Ghat.train.knn==0&Y.train==0)/sum(Y.train==0) # specificity

set.seed(2014)
model.knn <- knn(train=scale(X.train), test=scale(X.valid), cl=scale(Y.train), k=12, prob=T)
predclass.knn <- c(model.knn)-1 # convert factor to numeric classes

predprob.knn <- attr(model.knn, "prob") 

post.valid.knn <- predclass.knn*predprob.knn+(1-predclass.knn)*(1-predprob.knn)

Ghat.valid.knn <-  ifelse(post.valid.knn > sort(post.train.knn, decreasing=T)[201], 1, 0)

table(Ghat.valid.knn,Y.valid) # classification table

sum(abs(Ghat.valid.knn-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.knn==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.knn==0&Y.valid==0)/sum(Y.valid==0) # specificity
```

###KNN
The K-th nearest neighbor analysis was done by using cross-validation to choose the optimal k value.
The misclassification error is minimized when choosing k = 12, so each model is constructed using 12 as the
index.  
```{r echo=FALSE, comment=' '}
plot(mer, ylab="Misclassification Error Rate", xlab="K Value")
points(which.min(mer), mer[which.min(mer)], pch=9, col="red")
```
A cut-off posterior probability for the most likely interacted pairs in the KNN is 0.25.
```{r echo=FALSE, comment=' '}
table(Ghat.train.knn,Y.train)
table(Ghat.valid.knn,Y.valid)
summary.knn=data.frame(Classification=c("Training Data", "Validation Data"), ErrorRate=c("2.45%", "4.13%"), Sensitivity=c("46.7%","46.5%"), Specificity=c("98.9%", "97.4%"))
summary.knn
```
 For the validation set, the sensitivity of the KNN model is lower than 50% and the error rate is much higher at over 4%.

```{r include=FALSE}
library(gam)
model.gam <- gam(Y ~ s(X1,df=5) + s(X2,df=5)+ s(X3,df=5)+ s(X4,df=5)+ s(X5,df=5), data.train, family=binomial)
summary(model.gam)
post.train.gam <- model.gam$fitted.values # n.train posterior probabilities of Y=1

Ghat.train.gam <- ifelse(post.train.gam > sort(post.train.gam, decreasing=T)[201], 1, 0) # classification rule

table(Ghat.train.gam,Y.train) # classification table

sum(abs(Ghat.train.gam-Y.train))/n.train # training data classification error rate 

sum(Ghat.train.gam==1&Y.train==1)/sum(Y.train==1) # sensitivity 

sum(Ghat.train.gam==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.gam <- predict(model.gam, data.valid, type="response") # n.valid post probs

Ghat.valid.gam <- ifelse(post.valid.gam > sort(post.train.gam, decreasing=T)[201], 1, 0) 
table(Ghat.valid.gam,Y.valid) # classification table
sum(abs(Ghat.valid.gam-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.gam==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.gam==0&Y.valid==0)/sum(Y.valid==0) # specificity
```

###Logistic Regression Generalized Additive Model (GAM)
  The GAM was constructed using a smoothing spline with df=5 for each of the 5 predictors.
```{r echo=FALSE, comment=' '}
table(Ghat.train.gam,Y.train)
table(Ghat.valid.gam,Y.valid)
summary.gam=data.frame(Classification=c("Training Data", "Validation Data"), ErrorRate=c("3.0%", "7.26%"), Sensitivity=c("44.1%","55.8%"), Specificity=c("98.4%", "93.9%"))
summary.gam
```
Although the sensitivity of the GAM is comparable to that of the logistic and qda models, the error rate is much higher.  The specificity is nearly 5% lower in this model.

##Results
The following table shows the final validation set results for all 5 models.
```{r echo=FALSE, comment=' '}

summary.all=data.frame(ModelType=c("Logistic", "LDA", "QDA", "KNN", "GAM"), 
                       ErrorRate=c("3.85%", "3.26%", "3.41%", "4.13%", "7.26%"),
                       Sensitivity=c("55.8%","51.2%","55.8%","46.5%","55.8%"),
                       Specificity=c("97.4%", "98.2%", "97.9%", "97.4%", "93.9%"))
summary.all
```
##Conclusion
In summary, the sensitivity of the logistic, QDA, and GAM model are identical but the QDA has the highest 
specificity.  The QDA model is then concluded to be the "best" when predicting the interacting protein-protein
gene pairs.  Using the QDA model the following observations would be selected with the cut off posterior probability criteria used:
```{r echo=FALSE, comment=' '}
pred.interact= Ghat.valid.qda[Ghat.valid.qda==1]
 names(pred.interact)
```


\pagebreak

##APPENDIX 1 - R CODE

###Logistic Regression

```{r eval=FALSE}

model.logistic <- glm(Y ~., data.train, family=binomial("logit"))
summary(model.logistic)
post.train.logistic <- model.logistic$fitted.values # n.train posterior probabilities of Y=1

Ghat.train.logistic <-ifelse(post.train.logistic > sort(post.train.logistic, decreasing=T)[201], 1, 0) # classification rule
table(Ghat.train.logistic,Y.train) # classification table

sum(abs(Ghat.train.logistic-Y.train))/n.train # training data classification error rate 
sum(Ghat.train.logistic==1&Y.train==1)/sum(Y.train==1) # sensitivity
sum(Ghat.train.logistic==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.logistic <- predict(model.logistic, data.valid, type="response") # n.valid post probs
Ghat.valid.logistic <- ifelse(post.valid.logistic > sort(post.train.logistic, decreasing=T)[201], 1, 0) 
table(Ghat.valid.logistic,Y.valid)
sum(abs(Ghat.valid.logistic-Y.valid))/n.valid # classification error rate 
sum(Ghat.valid.logistic==1&Y.valid==1)/sum(Y.valid==1) # sensitivity
sum(Ghat.valid.logistic==0&Y.valid==0)/sum(Y.valid==0) # specificity
```



###  Linear Discriminant Analysis


```{r eval=FALSE}

library(MASS)
model.lda <- lda(Y ~ ., data.train)
model.lda
plot(model.lda) 
post.train.lda <- predict(model.lda)$posterior[,2] # n.train posterior probabilities of Y=1

sort(post.train.lda, decreasing=T)[201]
Ghat.train.lda <- ifelse(post.train.lda > sort(post.train.lda, decreasing=T)[201], 1, 0)  # classification rule

table(Ghat.train.lda,Y.train)
sum(abs(Ghat.train.lda-Y.train))/n.train # training data classification error rate
sum(Ghat.train.lda==1&Y.train==1)/sum(Y.train==1) # sensitivity

sum(Ghat.train.lda==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.lda <- predict(model.lda, data.valid)$posterior[,2] # n.valid posterior probabilities of Y=1
Ghat.valid.lda <- ifelse(post.valid.lda > sort(post.train.lda, decreasing=T)[201], 1, 0) 
table(Ghat.valid.lda,Y.valid) # classification table

sum(abs(Ghat.valid.lda-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.lda==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.lda==0&Y.valid==0)/sum(Y.valid==0) # specificity 
```


###Quadratic Discriminant Analysis
  

```{r eval=FALSE}
model.qda <- qda(Y ~ ., data.train)
post.train.qda <- predict(model.qda)$posterior[,2] 
Ghat.train.qda <- ifelse(post.train.qda > sort(post.train.qda, decreasing=T)[201], 1, 0)  # classification rule

table(Ghat.train.qda,Y.train) # classification table

sum(abs(Ghat.train.qda-Y.train))/n.train # training data classification error rate 

sum(Ghat.train.qda==1&Y.train==1)/sum(Y.train==1) # sensitivity 

sum(Ghat.train.qda==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.qda <- predict(model.qda, data.valid)$posterior[,2] # n.valid posterior probabilities of Y=1
Ghat.valid.qda <- ifelse(post.valid.qda > sort(post.train.qda, decreasing=T)[201], 1, 0) 
table(Ghat.valid.qda,Y.valid) # classification table

sum(abs(Ghat.valid.qda-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.qda==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.qda==0&Y.valid==0)/sum(Y.valid==0) # specificity 
```


###KNN

```{r eval=FALSE}
library(class)
mer <- rep(NA, 30) # misclassification error rates based on leave-one-out cross-validation

set.seed(2014)

for (i in 1:30) mer[i] <- sum((Y.train-(c(knn.cv(train=X.train, cl=Y.train, k=i))-1))^2)/n.train
plot(mer, ylab="Misclassification Error Rate", xlab="K Value")
points(which.min(mer), mer[which.min(mer)], pch=9, col="red")
which.min(mer) # minimum occurs at k=12
set.seed(2014)
trainmodel.knn <- knn(train=scale(X.train), test=scale(X.train), cl=scale(Y.train), k=12, prob=T)

trainpredclass.knn <- c(trainmodel.knn)-1 # convert factor to numeric classes

trainpredprob.knn <- attr(trainmodel.knn, "prob") # proportion of votes for winning class

post.train.knn <- trainpredclass.knn*trainpredprob.knn+(1-trainpredclass.knn)*(1-trainpredprob.knn)

Ghat.train.knn <- ifelse(post.train.knn > sort(post.train.knn, decreasing=T)[201], 1, 0) # classification rule

table(Ghat.train.knn,Y.train)
sum(abs(Ghat.train.knn-Y.train))/n.train # training data classification error rate
sum(Ghat.train.knn==1&Y.train==1)/sum(Y.train==1) # sensitivity 
sum(Ghat.train.knn==0&Y.train==0)/sum(Y.train==0) # specificity

set.seed(2014)
model.knn <- knn(train=scale(X.train), test=scale(X.valid), cl=scale(Y.train), k=12, prob=T)
predclass.knn <- c(model.knn)-1 # convert factor to numeric classes

predprob.knn <- attr(model.knn, "prob") 

post.valid.knn <- predclass.knn*predprob.knn+(1-predclass.knn)*(1-predprob.knn)

Ghat.valid.knn <-  ifelse(post.valid.knn > sort(post.train.knn, decreasing=T)[201], 1, 0)

table(Ghat.valid.knn,Y.valid) # classification table

sum(abs(Ghat.valid.knn-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.knn==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.knn==0&Y.valid==0)/sum(Y.valid==0) # specificity
```

###Logistic Regression Generalized Additive Model (GAM)
```{r eval=FALSE}
library(gam)
model.gam <- gam(Y ~ s(X1,df=5) + s(X2,df=5)+ s(X3,df=5)+ s(X4,df=5)+ s(X5,df=5), data.train, family=binomial)
summary(model.gam)
post.train.gam <- model.gam$fitted.values # n.train posterior probabilities of Y=1

Ghat.train.gam <- ifelse(post.train.gam > sort(post.train.gam, decreasing=T)[201], 1, 0) # classification rule

table(Ghat.train.gam,Y.train) # classification table

sum(abs(Ghat.train.gam-Y.train))/n.train # training data classification error rate 

sum(Ghat.train.gam==1&Y.train==1)/sum(Y.train==1) # sensitivity 

sum(Ghat.train.gam==0&Y.train==0)/sum(Y.train==0) # specificity

post.valid.gam <- predict(model.gam, data.valid, type="response") # n.valid post probs

Ghat.valid.gam <- ifelse(post.valid.gam > sort(post.train.gam, decreasing=T)[201], 1, 0) 
table(Ghat.valid.gam,Y.valid) # classification table
sum(abs(Ghat.valid.gam-Y.valid))/n.valid # classification error rate 

sum(Ghat.valid.gam==1&Y.valid==1)/sum(Y.valid==1) # sensitivity 

sum(Ghat.valid.gam==0&Y.valid==0)/sum(Y.valid==0) # specificity
```

